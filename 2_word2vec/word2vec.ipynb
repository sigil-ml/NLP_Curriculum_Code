{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset\n",
    "from transformers import BasicTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from IPython.display import display, HTML\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from datasets import DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "class OutputCapturer:\n",
    "    def __init__(self, height: int = 400):\n",
    "        self.height = height\n",
    "        self.output_buffer = io.StringIO()\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.original_stdout = sys.stdout\n",
    "        sys.stdout = self.output_buffer\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        sys.stdout = self.original_stdout\n",
    "        output_text = self.output_buffer.getvalue()\n",
    "        lines = output_text.splitlines()\n",
    "        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n",
    "        numbered_output = \"\\n\".join(numbered_lines)\n",
    "        html = f\"\"\"\n",
    "        <div style=\"\n",
    "            width: calc(100% - 40px);\n",
    "            max-height: {self.height}px; \n",
    "            overflow: auto;\n",
    "            background: rgb(0,0,0);\n",
    "            background: linear-gradient(0deg, rgba(10,10,10,1) 0%, rgba(31,31,31,1) 100%);\n",
    "            foreground-color: #E3D8F1;\n",
    "            padding: 20px;\n",
    "            font-family: Fira Code, monospace;\n",
    "            font-size: 14pt;\n",
    "            font-weight: 500;\n",
    "            border: solid 2px #E3D8F1;\n",
    "        \">\n",
    "            <pre style=\"\n",
    "                word-wrap: normal;   \n",
    "            \">{output_text}</pre>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to start with a large training set since we could have errors in our code that we should test on smaller amounts of data first. For this role I selected a small dataset consisting of short stories generated by `GPT-4`.\n",
    "\n",
    "We will use the `datasets` library from HuggingFace for convenience. This library will download the selected dataset and produce train-test splits automatically for us. We will use the default settings for loading the dataset since the other features (streaming, memory-pinning, etc.) are not very interesting for our use case. This will result in a dictionary object with two keys, one for the training set and the other for validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the validation set for later during the evaluation phase. There are over two million samples in the training set and over twenty thousand in the evaluation set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 2119719\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 21990\n",
       " }))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"], ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the samples. You can set the `batch_size` parameter below to control how many samples are displayed to the screen and set the `index` parameter to select which batch you want to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr style=\"border: none; border-top: 3px solid #3083DC; width: 100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style=\"border: none; border-top: 3px solid #3083DC; width: 100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\n",
      "\n",
      "Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style=\"border: none; border-top: 3px solid #3083DC; width: 100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2: One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don't want to play. I am cold and I don't feel fine.\"\n",
      "\n",
      "Fin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\n",
      "\n",
      "The sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together!\" And so, Fin and the crab played and became good friends.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style=\"border: none; border-top: 3px solid #3083DC; width: 100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3: Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees.\n",
      "\n",
      "One day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \"You are special because you have sweet cherries that everyone loves.\" The cherry tree started to feel a little better.\n",
      "\n",
      "As time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "index = 0\n",
    "sample = ds[\"train\"][index : index + batch_size][\"text\"]\n",
    "for i, s in enumerate(sample):\n",
    "    display(\n",
    "        HTML('<hr style=\"border: none; border-top: 3px solid #3083DC; width: 100%;\">')\n",
    "    )\n",
    "    print(f\"Sample {i}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each story consists of a few sentences without special characters or many numeric values. We will include these in the final training run when we switch to a larger training set, but for now this data will suffice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot feed the sequences of words found in the dataset directly into the model. We need to translate each sequence into atomic units of language we call _tokens_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the tokenizer used to train the model is also used for inference. If a different tokenizer is used then a word might be split in a way that is not expected by the model and will yield undesirable results. This is why in HuggingFace and other Machine Learning tools you will encounter tokenizers that are named after the model they are associated with (e.g. `T5Tokenizer`, `BERTTokenizer`, etc.). Just because a tokenizer was used to train a popular foundational language (such as BERT) doesn't mean you cannot use it for another NLP model if you are doing the pre-training of said model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple pre-tokenizer that splits on whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(ds[\"train\"][\"text\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"models/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'raining', 'a', 'token', 'izer', 'is', 'not', 'hard', 'when', 'you', 'have', 'good', 'libraries', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Training a tokenizer is not hard when you have good libraries.\"\n",
    "print(tokenizer.encode(sentence).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'day', ',', 'a', 'little', 'girl', 'named', 'Lily', 'found', 'a', 'needle', 'in', 'her', 'room', '.', 'She', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', '.', 'Lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', ',', 'so', 'she', 'could', 'sew', 'a', 'button', 'on', 'her', 'shirt', '.', 'Lily', 'went', 'to', 'her', 'mom', 'and', 'said', ',', '\"', 'Mom', ',', 'I', 'found', 'this', 'needle', '.', 'Can', 'you', 'share', 'it', 'with', 'me', 'and', 'sew', 'my', 'shirt', '?\"', 'Her', 'mom', 'smiled', 'and', 'said', ',', '\"', 'Yes', ',', 'Lily', ',', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt', '.\"', 'Together', ',', 'they', 'shared', 'the', 'needle', 'and', 'sewed', 'the', 'button', 'on', 'Lily', \"'\", 's', 'shirt', '.', 'It', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing', 'and', 'helping', 'each', 'other', '.', 'After', 'they', 'finished', ',', 'Lily', 'thanked', 'her', 'mom', 'for', 'sharing', 'the', 'needle', 'and', 'fixing', 'her', 'shirt', '.', 'They', 'both', 'felt', 'happy', 'because', 'they', 'had', 'shared', 'and', 'worked', 'together', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = ds[\"train\"][0][\"text\"]\n",
    "print(tokenizer.encode(sentence).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ds[\"train\"][0:4][\"text\"]\n",
    "encoded_batch = tokenizer.encode_batch(sentences)\n",
    "with OutputCapturer() as capturer:\n",
    "    for encoding in encoded_batch:\n",
    "        print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(samples):\n",
    "    return {\"tokens\": [b.tokens for b in tokenizer.encode_batch(samples[\"text\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2119719/2119719 [05:59<00:00, 5902.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = ds.map(encode, batched=True, batch_size=2_048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ds.map(encode, batched=True, batch_size=2_048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"\n",
       "            width: calc(100% - 40px);\n",
       "            max-height: 400px; \n",
       "            overflow: auto;\n",
       "            background: rgb(0,0,0);\n",
       "            background: linear-gradient(0deg, rgba(10,10,10,1) 0%, rgba(31,31,31,1) 100%);\n",
       "            foreground-color: #E3D8F1;\n",
       "            padding: 20px;\n",
       "            font-family: Fira Code, monospace;\n",
       "            font-size: 14pt;\n",
       "            font-weight: 500;\n",
       "            border: solid 2px #E3D8F1;\n",
       "        \">\n",
       "            <pre style=\"\n",
       "                word-wrap: normal;   \n",
       "            \">{'text': 'One day, a little girl named Lily found a needle in her room. She '\n",
       "         'knew it was difficult to play with it because it was sharp. Lily '\n",
       "         'wanted to share the needle with her mom, so she could sew a button '\n",
       "         'on her shirt.\\n'\n",
       "         '\\n'\n",
       "         'Lily went to her mom and said, \"Mom, I found this needle. Can you '\n",
       "         'share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, '\n",
       "         'Lily, we can share the needle and fix your shirt.\"\\n'\n",
       "         '\\n'\n",
       "         \"Together, they shared the needle and sewed the button on Lily's \"\n",
       "         'shirt. It was not difficult for them because they were sharing and '\n",
       "         'helping each other. After they finished, Lily thanked her mom for '\n",
       "         'sharing the needle and fixing her shirt. They both felt happy '\n",
       "         'because they had shared and worked together.',\n",
       " 'tokens': ['One', 'day', ',', 'a', 'little', 'girl', 'named', 'Lily', 'found',\n",
       "            'a', 'needle', 'in', 'her', 'room', '.', 'She', 'knew', 'it', 'was',\n",
       "            'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was',\n",
       "            'sharp', '.', 'Lily', 'wanted', 'to', 'share', 'the', 'needle',\n",
       "            'with', 'her', 'mom', ',', 'so', 'she', 'could', 'sew', 'a',\n",
       "            'button', 'on', 'her', 'shirt', '.', 'Lily', 'went', 'to', 'her',\n",
       "            'mom', 'and', 'said', ',', '\"', 'Mom', ',', 'I', 'found', 'this',\n",
       "            'needle', '.', 'Can', 'you', 'share', 'it', 'with', 'me', 'and',\n",
       "            'sew', 'my', 'shirt', '?\"', 'Her', 'mom', 'smiled', 'and', 'said',\n",
       "            ',', '\"', 'Yes', ',', 'Lily', ',', 'we', 'can', 'share', 'the',\n",
       "            'needle', 'and', 'fix', 'your', 'shirt', '.\"', 'Together', ',',\n",
       "            'they', 'shared', 'the', 'needle', 'and', 'sewed', 'the', 'button',\n",
       "            'on', 'Lily', \"'\", 's', 'shirt', '.', 'It', 'was', 'not',\n",
       "            'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing',\n",
       "            'and', 'helping', 'each', 'other', '.', 'After', 'they', 'finished',\n",
       "            ',', 'Lily', 'thanked', 'her', 'mom', 'for', 'sharing', 'the',\n",
       "            'needle', 'and', 'fixing', 'her', 'shirt', '.', 'They', 'both',\n",
       "            'felt', 'happy', 'because', 'they', 'had', 'shared', 'and',\n",
       "            'worked', 'together', '.']}\n",
       "</pre>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with OutputCapturer() as capturer:\n",
    "    pprint(train_ds[0], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\"train\": train_ds, \"test\": test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/dwalker/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mgit version 2.34.1\u001b[0m\n",
      "\u001b[90mgit-lfs/3.0.2 (GitHub; linux amd64; go 1.18.1)\u001b[0m\n",
      "\n",
      "You are about to create \u001b[1mdatasets/athena-ml/gpt4_short_stories_with_tokens\u001b[0m\n",
      "Proceed? [Y/n] 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-66d7b06e-67e498be1aba9301452914f2;49df577c-64b3-453f-9854-f73051d02a35)\n",
      "\n",
      "You already created this dataset repo\n",
      "\u001b[1m\u001b[31m{\"error\":\"You already created this dataset repo\",\"url\":\"https://huggingface.co/datasets/athena-ml/gpt4_short_stories_with_tokens\"}\u001b[0m\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | huggingface-cli repo create gpt4_short_stories_with_tokens --type dataset --organization athena-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 46.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.28ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 44.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 46.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 46.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 193/193 [00:04<00:00, 45.39ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 11/11 [00:50<00:00,  4.55s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 22/22 [00:00<00:00, 47.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/athena-ml/gpt4_short_stories_with_tokens/commit/09c90ea6803d3bade5c4764bb7df059122c36d9a', commit_message='Upload dataset', commit_description='', oid='09c90ea6803d3bade5c4764bb7df059122c36d9a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"athena-ml/gpt4_short_stories_with_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_CBOW(L.LightningModule):\n",
    "    \"\"\"Implements the Continuous Bag of Words Word2Vec model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.neighborhood_size = 2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.projection_dim = embedding_dim\n",
    "        self.projection_layer = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim, max_norm=1\n",
    "        )\n",
    "        self.linear_layer = nn.Linear(\n",
    "            in_features=embedding_dim, out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection_layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear_layer(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.vocab_size), y.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "\n",
    "\n",
    "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n",
    "    columns = len(val_dict)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns * 3, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in sorted(val_dict.keys()):\n",
    "        key_ax = ax[fig_index % columns]\n",
    "        sns.histplot(\n",
    "            val_dict[key],\n",
    "            ax=key_ax,\n",
    "            color=color,\n",
    "            bins=50,\n",
    "            stat=stat,\n",
    "            kde=use_kde and ((val_dict[key].max() - val_dict[key].min()) > 1e-8),\n",
    "        )  # Only plot kde if there is variance\n",
    "        hidden_dim_str = (\n",
    "            r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0])\n",
    "            if len(val_dict[key].shape) > 1\n",
    "            else \"\"\n",
    "        )\n",
    "        key_ax.set_title(f\"{key} {hidden_dim_str}\")\n",
    "        if xlabel is not None:\n",
    "            key_ax.set_xlabel(xlabel)\n",
    "        fig_index += 1\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    return fig\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "def visualize_weight_distribution(model, color=\"C0\"):\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        key_name = f\"Layer {name.split('.')[1]}\"\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "\n",
    "    # Plotting\n",
    "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
    "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "def visualize_gradients(model, color=\"C0\", print_variance=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        net: Object of class BaseNetwork\n",
    "        color: Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    model.zero_grad()\n",
    "    preds = model(imgs)\n",
    "    loss = F.cross_entropy(\n",
    "        preds, labels\n",
    "    )  # Same as nn.CrossEntropyLoss, but as a function instead of module\n",
    "    loss.backward()\n",
    "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
    "    grads = {\n",
    "        name: params.grad.view(-1).cpu().clone().numpy()\n",
    "        for name, params in model.named_parameters()\n",
    "        if \"weight\" in name\n",
    "    }\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Plotting\n",
    "    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n",
    "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in sorted(grads.keys()):\n",
    "            print(f\"{key} - Variance: {np.var(grads[key])}\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "def visualize_activations(model, color=\"C0\", print_variance=False):\n",
    "    model.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    feats = imgs.view(imgs.shape[0], -1)\n",
    "    activations = {}\n",
    "    with torch.no_grad():\n",
    "        for layer_index, layer in enumerate(model.layers):\n",
    "            feats = layer(feats)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[f\"Layer {layer_index}\"] = (\n",
    "                    feats.view(-1).detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "    # Plotting\n",
    "    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n",
    "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if print_variance:\n",
    "        for key in sorted(activations.keys()):\n",
    "            print(f\"{key} - Variance: {np.var(activations[key])}\")\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_init(model, fill=0.0):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.fill_(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vocab_size=100, embedding_dim=10)\n",
    "const_init(model, fill=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 1667."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_curr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
