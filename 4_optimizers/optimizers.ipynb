{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1796d35afcc29de",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46673313531d4f69",
   "metadata": {},
   "source": [
    "Covers various topics on optimization in the context of Deep Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:46.768726Z",
     "start_time": "2024-09-23T01:24:45.226971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import random as py_random\n",
    "from jax import grad\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import helpers as hp\n",
    "from optimizers import sgd, sgd_batched, RMSProp, Adam\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0891cd20d7ba56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:46.844779Z",
     "start_time": "2024-09-23T01:24:46.783400Z"
    }
   },
   "outputs": [],
   "source": [
    "pio.templates.default = \"ggplot2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f6f1a47a523dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.447617Z",
     "start_time": "2024-09-23T01:24:47.190497Z"
    }
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc433f704dc94fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.493556Z",
     "start_time": "2024-09-23T01:24:48.474927Z"
    }
   },
   "outputs": [],
   "source": [
    "x = jnp.arange(0, 10, 0.1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64efd707760178f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.526105Z",
     "start_time": "2024-09-23T01:24:48.522118Z"
    }
   },
   "outputs": [],
   "source": [
    "f_dx = grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4cb4730e9f7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:49.484951Z",
     "start_time": "2024-09-23T01:24:48.554379Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"x\"] = list(x)\n",
    "df[\"f\"] = list(f(x))\n",
    "df[\"df\"] = list(f_dx(_x) for _x in x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3df1a3270fc41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.360459Z",
     "start_time": "2024-09-23T01:24:49.553208Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"x\", y=[\"f\", \"df\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a3cb35169838b",
   "metadata": {},
   "source": [
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\hat{y}\\_i \\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da203e26c56072",
   "metadata": {},
   "source": [
    "If we naively define the MSE function it would look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d46ebffc839357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.616335Z",
     "start_time": "2024-09-23T01:24:50.611855Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_naive(true, pred):\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115cc5348a4ba59",
   "metadata": {},
   "source": [
    "But if we want to take the derivative of this function using JAX's `grad` function, then we need to explicitly state all the variables in the function signature as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189860d9eb37f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.726677Z",
     "start_time": "2024-09-23T01:24:50.722465Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(theta, x, true):\n",
    "    pred = x.dot(theta)\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de8d2030633136",
   "metadata": {},
   "source": [
    "Suppose $\\hat{y} = 2*x$. It is typical to use $w$ or $\\theta$ as the parameter of our function, we will stick with $\\theta$ as it extends naturally to probability. In our function we only have one parameter, hence $\\theta = 2$.\n",
    "\n",
    "We are interested in perturbing this parameter to minimize the loss over our observed (or true) sample set. To do this we will measure the impact the parameter had on the loss function, seeking to minimize this impact. To measure this impact, lets take the derivate of MSE using our parameterized function. First we will re-write the MSE loss function and then take the derivate with respect to the parameter $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MSE} &= \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta x \\right)^2 \\\\\n",
    "D_{\\theta} \\text{MSE} &= D_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta x  \\right)^2 \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^n 2 \\left(y_i - \\theta x \\right) * (-1)x \\\\\n",
    "&=-\\frac{2}{n} \\sum_{i=1}^n \\left(y_i - \\theta x \\right)x\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c29b415be0f62a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.842340Z",
     "start_time": "2024-09-23T01:24:50.837670Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_dx_man(theta, x, true):\n",
    "    return -2 * jnp.mean((true - theta * x) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ef5be255c9074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.067637Z",
     "start_time": "2024-09-23T01:24:50.893843Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_theta_man = mean_squared_error_dx_man(2.0, x, f(x)).item()\n",
    "print(f\"Loss (theta = 2): {loss_theta_man:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13948aa3cb0b5b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.108423Z",
     "start_time": "2024-09-23T01:24:51.104150Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_squared_error_d_theta = grad(mean_squared_error, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e86efe1843ebc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.438024Z",
     "start_time": "2024-09-23T01:24:51.150798Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_theta = mean_squared_error_d_theta(2.0, x, f(x)).item()\n",
    "print(f\"Loss (theta = 2): {loss_theta:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e424369de863cee",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66a4ad8ebc79ab",
   "metadata": {},
   "source": [
    "$$ \\theta_{t+1} = \\theta_t - \\alpha D_\\theta \\text{MSE} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832adc9d57d1a1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.021525Z",
     "start_time": "2024-09-23T01:24:51.485871Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "theta = random.uniform(key)\n",
    "n_iterations = 20\n",
    "fig = go.Figure()\n",
    "steps = []\n",
    "x = jnp.arange(0, 10, 0.1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 4 * x**3 + 3\n",
    "\n",
    "\n",
    "print(f\"Initial theta: {theta}\")\n",
    "\n",
    "fig.add_trace(go.Scatter(visible=False, name=\"True Function\", x=x, y=f(x)))\n",
    "\n",
    "prev_loss = 0\n",
    "for i in range(n_iterations):\n",
    "    ###########################################################################\n",
    "    theta -= alpha * mean_squared_error_d_theta(theta, x, f(x))\n",
    "    loss = mean_squared_error(theta, x, f(x)).item()\n",
    "    if jnp.abs(loss - prev_loss) < 1e-6:\n",
    "        print(f\"Converged at iteration {i + 1}\")\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    ###########################################################################\n",
    "    fig.add_trace(\n",
    "        go.Scatter(visible=False, name=f\"Iteration {(i + 1):4d}\", x=x, y=theta * x)\n",
    "    )\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [False] * (n_iterations + 1)},\n",
    "            {\"title\": \"SGD Iteration: \" + str(i + 1)},\n",
    "        ],\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][0] = True\n",
    "    step[\"args\"][0][\"visible\"][i + 1] = True\n",
    "    steps.append(step)\n",
    "    ###########################################################################\n",
    "    print(f\"Iteration {(i + 1):4d}: y_pred = [{theta:.3f}][x1].T, loss = {loss:.3f}\")\n",
    "\n",
    "sliders = [\n",
    "    dict(\n",
    "        active=n_iterations,\n",
    "        currentvalue={\"prefix\": \"SGD: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps,\n",
    "    )\n",
    "]\n",
    "\n",
    "fig.data[0].visible = True\n",
    "fig.data[1].visible = True\n",
    "fig.update_layout(sliders=sliders)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1dff50900e840f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.508227Z",
     "start_time": "2024-09-23T01:24:52.084880Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_2d(x, y):\n",
    "    return 3 * x**2 + 9 * y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890acffbeeb5e8cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.539775Z",
     "start_time": "2024-09-23T01:24:52.535278Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(theta1, theta2, f_pred, x, y, true):\n",
    "    pred = f_pred(theta1, theta2, x, y)\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375504c4",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a80f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='sgd.jpeg', retina=True, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d47b263023fd6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-23T01:24:52.607181Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "f_pred = lambda theta1, theta2, x, y: (  # noqa: E731\n",
    "    theta1 * x**2 + theta2 * y**2\n",
    ")\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 50\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = sgd(\n",
    "    f_pred,\n",
    "    lr,\n",
    "    max_n_iterations,\n",
    "    mse,\n",
    "    examples,\n",
    "    convergence_criteria,\n",
    "    key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_true=f_2d,\n",
    "    f_pred=f_pred,\n",
    "    loss_fn=mse,\n",
    "    theta1s=sgd_history[\"theta1s\"],\n",
    "    theta2s=sgd_history[\"theta2s\"],\n",
    "    losses=sgd_history[\"losses\"],\n",
    "    n_iterations=sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604f3c0",
   "metadata": {},
   "source": [
    "### SGD (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b4bb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "f_pred = lambda theta1, theta2, x, y: (  # noqa: E731\n",
    "    theta1 * x**2 + theta2 * y**2\n",
    ")\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 50\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = sgd_batched(\n",
    "    f_pred,\n",
    "    lr,\n",
    "    16,\n",
    "    max_n_iterations,\n",
    "    mse,\n",
    "    examples,\n",
    "    convergence_criteria,\n",
    "    key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_true=f_2d,\n",
    "    f_pred=f_pred,\n",
    "    loss_fn=mse,\n",
    "    theta1s=sgd_history[\"theta1s\"],\n",
    "    theta2s=sgd_history[\"theta2s\"],\n",
    "    losses=sgd_history[\"losses\"],\n",
    "    n_iterations=sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac78de",
   "metadata": {},
   "source": [
    "### RMS Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2993e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='rmsprop.jpeg', retina=True, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cc30530",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "max_n_iterations = 500\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "f_pred = lambda theta1, theta2, x, y: (  # noqa: E731\n",
    "    theta1 * x**2 + theta2 * y**2\n",
    ")\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = RMSProp(\n",
    "    f_pred=f_pred,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    smoothing_constant=0.99,\n",
    "    momentum=0.9,\n",
    "    centered=False,\n",
    "    max_n_iterations=max_n_iterations,\n",
    "    loss_fn=mse,\n",
    "    examples=examples,\n",
    "    convergence_criteria=convergence_criteria,\n",
    "    key=key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_true=f_2d,\n",
    "    f_pred=f_pred,\n",
    "    loss_fn=mse,\n",
    "    theta1s=sgd_history[\"theta1s\"],\n",
    "    theta2s=sgd_history[\"theta2s\"],\n",
    "    losses=sgd_history[\"losses\"],\n",
    "    n_iterations=sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686fe9b",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcde7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='adam.jpeg', retina=True, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ae71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "max_n_iterations = 5000\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "f_pred = lambda theta1, theta2, x, y: (  # noqa: E731\n",
    "    theta1 * x**2 + theta2 * y**2\n",
    ")\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = Adam(\n",
    "    f_pred=f_pred,\n",
    "    lr=lr,\n",
    "    weight_decay=0,\n",
    "    betas=(0.9, 0.999),\n",
    "    max_n_iterations=max_n_iterations,\n",
    "    loss_fn=mse,\n",
    "    examples=examples,\n",
    "    convergence_criteria=convergence_criteria,\n",
    "    key=key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_true=f_2d,\n",
    "    f_pred=f_pred,\n",
    "    loss_fn=mse,\n",
    "    theta1s=sgd_history[\"theta1s\"],\n",
    "    theta2s=sgd_history[\"theta2s\"],\n",
    "    losses=sgd_history[\"losses\"],\n",
    "    n_iterations=sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cee1a",
   "metadata": {},
   "source": [
    "#### Ackley Function\n",
    "\n",
    "https://en.wikipedia.org/wiki/Ackley_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a64349",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f(x, y) &= -20\\exp{\\left[-0.2\\sqrt{0.5(x^2+y^2)} \\right]} \\\\\n",
    "        &-\\exp{\\left[0.5*(\\cos(2 \\pi x) + \\cos(2 \\pi y)) \\right] + e + 20}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ca89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackley_fn(x, y):\n",
    "    return (\n",
    "        -20 * jnp.exp(-0.2 * jnp.sqrt(0.5 * (x**2 + y**2)))\n",
    "        - jnp.exp(0.5 * (jnp.cos(2 * jnp.pi * x) + jnp.cos(2 * jnp.pi * y)))\n",
    "        + jnp.e\n",
    "        + 20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23778e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.arange(-10, 10, 0.1)\n",
    "ys = jnp.arange(-10, 10, 0.1)\n",
    "zs = jnp.array([ackley_fn(x, ys) for x in xs])\n",
    "true_function_surface = go.Surface(\n",
    "    z=zs, x=xs, y=ys, colorscale=\"Blues\", showscale=False\n",
    ")\n",
    "\n",
    "ack_fig = go.Figure()\n",
    "ack_fig.update_layout(height=800)\n",
    "ack_fig.add_trace(true_function_surface)\n",
    "ack_fig.update_layout(title=\"Ackley Function\")\n",
    "ack_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3958fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_converged(true_val: float, test_val: float, convergence_criterion: float = 1e-2) -> bool:\n",
    "    \"\"\"Test if the parameter is within the convergence criteria.\"\"\"\n",
    "    return jnp.abs(true_val - test_val) < convergence_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "lr = 1e-1\n",
    "max_n_iterations = 20\n",
    "convergence_criteria = 1e-2\n",
    "\n",
    "loss = 0\n",
    "x_grad = grad(ackley_fn, argnums=0)\n",
    "y_grad = grad(ackley_fn, argnums=1)\n",
    "gradients = (x_grad, y_grad)\n",
    "min_x, min_y = 0, 0\n",
    "x, y = 4., 4.\n",
    "x_path, y_path = [], []\n",
    "\n",
    "for i in range(max_n_iterations):\n",
    "    x -= lr * x_grad(x, y)\n",
    "    y -= lr * y_grad(x, y)\n",
    "    x_path.append(x)\n",
    "    y_path.append(y)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i:2d} | (x, y) = ({x:.3f}, {y:.3f})\")\n",
    "\n",
    "    if has_converged(min_x, x) and has_converged(min_y, y):\n",
    "        print(f\"Converged at iteration {i + 1}\")\n",
    "        max_n_iterations = i\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d80371",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.create_optimizer_figure_true(\n",
    "    fn=ackley_fn,\n",
    "    title=\"Ackley Function (SGD)\",\n",
    "    x_path=x_path,\n",
    "    y_path=y_path,\n",
    "    n_iterations=max_n_iterations,\n",
    "    perf_profiling=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "lr = 1e-2\n",
    "weight_decay = 0.7\n",
    "max_n_iterations = 1000\n",
    "convergence_criteria = 1e-2\n",
    "\n",
    "loss = 0\n",
    "betas=(0.99, 0.999)\n",
    "beta1, beta2 = betas\n",
    "min_x, min_y = 0, 0\n",
    "variables = [4., 4.]\n",
    "x_path, y_path = [], []\n",
    "x_path.append(variables[0])\n",
    "y_path.append(variables[1])\n",
    "n_params = 2\n",
    "first_moments = [0, 0]\n",
    "second_moments = [0, 0]\n",
    "bias_corrected_first_moments = [0, 0]\n",
    "bias_corrected_second_moments = [0, 0]\n",
    "\n",
    "\n",
    "for i in range(1, max_n_iterations):\n",
    "\n",
    "    for param_idx in range(n_params):\n",
    "            gradient = grad(ackley_fn, argnums=param_idx)(*variables)\n",
    "\n",
    "            v = variables[param_idx]\n",
    "            fm = first_moments[param_idx]\n",
    "            sm = second_moments[param_idx]\n",
    "            bc_fm = bias_corrected_first_moments[param_idx]\n",
    "            bc_sm = bias_corrected_second_moments[param_idx]\n",
    "\n",
    "            # Update the parameters\n",
    "\n",
    "            if weight_decay != 0:\n",
    "                gradient += weight_decay * theta\n",
    "\n",
    "            fm = beta1 * fm + (1 - beta1) * gradient\n",
    "            sm = beta2 * sm + (1 - beta2) * gradient**2\n",
    "\n",
    "            # bias corrections\n",
    "            bc_fm = fm / (1 - beta1**i)\n",
    "            bc_sm = sm / (1 - beta2**i)\n",
    "\n",
    "            # 1e-8 is added to avoid division by zero\n",
    "            v -= lr * bc_fm / (jnp.sqrt(bc_sm) + 1e-8)\n",
    "\n",
    "            variables[param_idx] = v\n",
    "            first_moments[param_idx] = fm\n",
    "            second_moments[param_idx] = sm\n",
    "            bias_corrected_first_moments[param_idx] = bc_fm\n",
    "            bias_corrected_second_moments[param_idx] = bc_sm\n",
    "\n",
    "    x_path.append(variables[0])\n",
    "    y_path.append(variables[1])\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Iteration: {i:2d} | (x, y) = ({variables[0]:.3f}, {variables[1]:.3f})\")\n",
    "\n",
    "    if has_converged(min_x, variables[0]) and has_converged(min_y, variables[1]):\n",
    "        print(f\"Converged at iteration {i + 1}\")\n",
    "        max_n_iterations = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa94e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.create_optimizer_figure_true(\n",
    "    fn=ackley_fn,\n",
    "    title=\"Ackley Function (Adam)\",\n",
    "    x_path=x_path,\n",
    "    y_path=y_path,\n",
    "    n_iterations=max_n_iterations,\n",
    "    perf_profiling=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse_fn = lambda x, y: x**2 / 3**2 + y**2 / 9**2 - 1\n",
    "\n",
    "xs = jnp.arange(-10, 10, 0.1)\n",
    "ys = jnp.arange(-10, 10, 0.1)\n",
    "zs = jnp.array([ellipse_fn(x, ys) for x in xs])\n",
    "true_function_contour = go.Contour(\n",
    "    z=zs, x=xs, y=ys, colorscale=\"Blues\", showscale=False\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.update_layout(height=800)\n",
    "fig.add_trace(true_function_contour)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba87d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "lr = 1e-1\n",
    "max_n_iterations = 1000\n",
    "convergence_criteria = 1e-2\n",
    "\n",
    "loss = 0\n",
    "x_grad = grad(ellipse_fn, argnums=0)\n",
    "y_grad = grad(ellipse_fn, argnums=1)\n",
    "gradients = (x_grad, y_grad)\n",
    "min_x, min_y = 0, 0\n",
    "x, y = -10., 8.\n",
    "x_path, y_path = [], []\n",
    "\n",
    "for i in range(max_n_iterations):\n",
    "    x -= lr * x_grad(x, y)\n",
    "    y -= lr * y_grad(x, y)\n",
    "    x_path.append(x)\n",
    "    y_path.append(y)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i:2d} | (x, y) = ({x:.3f}, {y:.3f})\")\n",
    "\n",
    "    if has_converged(min_x, x) and has_converged(min_y, y):\n",
    "        print(f\"Converged at iteration {i + 1}\")\n",
    "        max_n_iterations = i\n",
    "        break\n",
    "\n",
    "hp.create_optimizer_figure_true(\n",
    "    fn=ellipse_fn,\n",
    "    title=\"Ellipse (SGD)\",\n",
    "    x_path=x_path,\n",
    "    y_path=y_path,\n",
    "    n_iterations=max_n_iterations,\n",
    "    graph_type=\"contour\",\n",
    "    perf_profiling=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e67b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
