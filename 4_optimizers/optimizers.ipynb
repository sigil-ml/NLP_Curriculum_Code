{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1796d35afcc29de",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46673313531d4f69",
   "metadata": {},
   "source": [
    "Covers various topics on optimization in the context of Deep Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:46.768726Z",
     "start_time": "2024-09-23T01:24:45.226971Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import random as py_random\n",
    "from jax import grad\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import helpers as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0891cd20d7ba56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:46.844779Z",
     "start_time": "2024-09-23T01:24:46.783400Z"
    }
   },
   "outputs": [],
   "source": [
    "pio.templates.default = \"ggplot2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f6f1a47a523dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.447617Z",
     "start_time": "2024-09-23T01:24:47.190497Z"
    }
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc433f704dc94fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.493556Z",
     "start_time": "2024-09-23T01:24:48.474927Z"
    }
   },
   "outputs": [],
   "source": [
    "x = jnp.arange(0, 10, 0.1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64efd707760178f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:48.526105Z",
     "start_time": "2024-09-23T01:24:48.522118Z"
    }
   },
   "outputs": [],
   "source": [
    "f_dx = grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4cb4730e9f7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:49.484951Z",
     "start_time": "2024-09-23T01:24:48.554379Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"x\"] = list(x)\n",
    "df[\"f\"] = list(f(x))\n",
    "df[\"df\"] = list(f_dx(_x) for _x in x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3df1a3270fc41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.360459Z",
     "start_time": "2024-09-23T01:24:49.553208Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"x\", y=[\"f\", \"df\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a3cb35169838b",
   "metadata": {},
   "source": [
    "$$ \\text{MSE} = \\frac{1}{n} \\sum\\_{i=1}^n \\left(y_i - \\hat{y}\\_i \\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da203e26c56072",
   "metadata": {},
   "source": [
    "If we naively define the MSE function it would look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d46ebffc839357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.616335Z",
     "start_time": "2024-09-23T01:24:50.611855Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_naive(true, pred):\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115cc5348a4ba59",
   "metadata": {},
   "source": [
    "But if we want to take the derivative of this function using JAX's `grad` function, then we need to explicitly state all the variables in the function signature as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189860d9eb37f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.726677Z",
     "start_time": "2024-09-23T01:24:50.722465Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(theta, x, true):\n",
    "    pred = x.dot(theta)\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de8d2030633136",
   "metadata": {},
   "source": [
    "Suppose $\\hat{y} = 2*x$. It is typical to use $w$ or $\\theta$ as the parameter of our function, we will stick with $\\theta$ as it extends naturally to probability. In our function we only have one parameter, hence $\\theta = 2$.\n",
    "\n",
    "We are interested in perturbing this parameter to minimize the loss over our observed (or true) sample set. To do this we will measure the impact the parameter had on the loss function, seeking to minimize this impact. To measure this impact, lets take the derivate of MSE using our parameterized function. First we will re-write the MSE loss function and then take the derivate with respect to the parameter $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MSE} &= \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta x \\right)^2 \\\\\n",
    "D_{\\theta} \\text{MSE} &= D_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta x  \\right)^2 \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^n 2 \\left(y_i - \\theta x \\right) * (-1)x \\\\\n",
    "&=-\\frac{2}{n} \\sum_{i=1}^n \\left(y_i - \\theta x \\right)x\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c29b415be0f62a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:50.842340Z",
     "start_time": "2024-09-23T01:24:50.837670Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_dx_man(theta, x, true):\n",
    "    return -2 * jnp.mean((true - theta * x) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ef5be255c9074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.067637Z",
     "start_time": "2024-09-23T01:24:50.893843Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_theta_man = mean_squared_error_dx_man(2.0, x, f(x)).item()\n",
    "print(f\"Loss (theta = 2): {loss_theta_man:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13948aa3cb0b5b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.108423Z",
     "start_time": "2024-09-23T01:24:51.104150Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_squared_error_d_theta = grad(mean_squared_error, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e86efe1843ebc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:51.438024Z",
     "start_time": "2024-09-23T01:24:51.150798Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_theta = mean_squared_error_d_theta(2.0, x, f(x)).item()\n",
    "print(f\"Loss (theta = 2): {loss_theta:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e424369de863cee",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66a4ad8ebc79ab",
   "metadata": {},
   "source": [
    "$$ \\theta*{t+1} = \\theta_t - \\alpha D*\\theta \\text{MSE} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832adc9d57d1a1fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.021525Z",
     "start_time": "2024-09-23T01:24:51.485871Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "theta = random.uniform(key)\n",
    "n_iterations = 20\n",
    "fig = go.Figure()\n",
    "steps = []\n",
    "x = jnp.arange(0, 10, 0.1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 4 * x**3 + 3\n",
    "\n",
    "\n",
    "print(f\"Initial theta: {theta}\")\n",
    "\n",
    "fig.add_trace(go.Scatter(visible=False, name=\"True Function\", x=x, y=f(x)))\n",
    "\n",
    "prev_loss = 0\n",
    "for i in range(n_iterations):\n",
    "    ###########################################################################\n",
    "    theta -= alpha * mean_squared_error_d_theta(theta, x, f(x))\n",
    "    loss = mean_squared_error(theta, x, f(x)).item()\n",
    "    if jnp.abs(loss - prev_loss) < 1e-6:\n",
    "        print(f\"Converged at iteration {i + 1}\")\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    ###########################################################################\n",
    "    fig.add_trace(\n",
    "        go.Scatter(visible=False, name=f\"Iteration {(i + 1):4d}\", x=x, y=theta * x)\n",
    "    )\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [False] * (n_iterations + 1)},\n",
    "            {\"title\": \"SGD Iteration: \" + str(i + 1)},\n",
    "        ],\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][0] = True\n",
    "    step[\"args\"][0][\"visible\"][i + 1] = True\n",
    "    steps.append(step)\n",
    "    ###########################################################################\n",
    "    print(f\"Iteration {(i + 1):4d}: y_pred = [{theta:.3f}][x1].T, loss = {loss:.3f}\")\n",
    "\n",
    "sliders = [\n",
    "    dict(\n",
    "        active=n_iterations,\n",
    "        currentvalue={\"prefix\": \"SGD: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps,\n",
    "    )\n",
    "]\n",
    "\n",
    "fig.data[0].visible = True\n",
    "fig.data[1].visible = True\n",
    "fig.update_layout(sliders=sliders)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1dff50900e840f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.508227Z",
     "start_time": "2024-09-23T01:24:52.084880Z"
    }
   },
   "outputs": [],
   "source": [
    "# def f_2d(x, y):\n",
    "#     return 3 * x**2 + 9 * y**2\n",
    "\n",
    "\n",
    "def f_2d(x, y):\n",
    "    return (\n",
    "        -20 * jnp.exp(-0.2 * jnp.sqrt(0.5 * (x**2 + y**2)))\n",
    "        - jnp.exp(0.5 * (jnp.cos(2 * jnp.pi * x) + jnp.cos(2 * jnp.pi * y)))\n",
    "        + jnp.e\n",
    "        + 20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f1024",
   "metadata": {},
   "source": [
    "#### Ackley Function\n",
    "\n",
    "https://en.wikipedia.org/wiki/Ackley_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1d1cb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f(x, y) &= -20\\exp{\\left[-0.2\\sqrt{0.5(x^2+y^2)} \\right]} \\\\\n",
    "        &-\\exp{\\left[0.5*(\\cos(2 \\pi x) + \\cos(2 \\pi y)) \\right] + e + 20}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "890acffbeeb5e8cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.539775Z",
     "start_time": "2024-09-23T01:24:52.535278Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_2d(theta1, theta2, x, y, true):\n",
    "    pred = theta1 * x**2 + theta2 * y**2 + jnp.exp(1) + theta2\n",
    "    return jnp.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375504c4",
   "metadata": {},
   "source": [
    "#### SGD\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760fb4cb73b34340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T01:24:52.579251Z",
     "start_time": "2024-09-23T01:24:52.567345Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(\n",
    "    lr: float,\n",
    "    max_n_iterations: int,\n",
    "    true_thetas: tuple[float | int, float | int],\n",
    "    loss_fn,\n",
    "    examples: list[tuple[float, float, float]],\n",
    "    convergence_criteria: float,\n",
    "    key,\n",
    "    return_history: bool = False,\n",
    ") -> dict | tuple[float, float]:\n",
    "    r\"\"\"Perform Stochastic Gradient Descent.\n",
    "\n",
    "    Args:\n",
    "        lr: The learning rate.\n",
    "        max_n_iterations: The maximum number of iterations.\n",
    "        true_thetas: The true thetas.\n",
    "        loss_fn: The loss function.\n",
    "        examples: The examples.\n",
    "        convergence_criteria: The convergence criteria.\n",
    "        key: The random key.\n",
    "        return_history: Whether to return the history.\n",
    "\n",
    "    Returns:\n",
    "        theta1s: The theta1 values recorded during optimization.\n",
    "        theta2s: The theta2 values recorded during optimization.\n",
    "        losses: The losses recorded during optimization.\n",
    "        f_preds: The predicted functions.\n",
    "        max_n_iterations: The maximum number of iterations\n",
    "\n",
    "        or\n",
    "\n",
    "        theta1: The final theta1 value.\n",
    "        theta2: The final theta2 value\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Running SGD with learning rate: {lr} and max iterations: {max_n_iterations}\"\n",
    "    )\n",
    "    print(f\"True thetas: {true_thetas}\")\n",
    "\n",
    "    loss_theta1 = grad(loss_fn, argnums=0)\n",
    "    loss_theta2 = grad(loss_fn, argnums=1)\n",
    "    theta1, theta2 = random.uniform(key), random.uniform(key)\n",
    "    prev_loss = 0\n",
    "    theta1s, theta2s, losses, f_preds = [], [], [], []\n",
    "    for i in range(max_n_iterations):\n",
    "        key, subkey = random.split(key)\n",
    "        idx = random.randint(subkey, (1,), 0, len(examples))\n",
    "        x, y, z = examples[idx[0]]\n",
    "        theta1 -= lr * loss_theta1(theta1, theta2, x, y, z)\n",
    "        theta2 -= lr * loss_theta2(theta1, theta2, x, y, z)\n",
    "        loss = loss_2d(theta1, theta2, x, y, z).item()\n",
    "        theta1s.append(theta1.item())\n",
    "        theta2s.append(theta2.item())\n",
    "        losses.append(loss)\n",
    "        f_preds.append(\n",
    "            lambda x, y, theta1, theta2: theta1 * x**2\n",
    "            + theta2 * y**2\n",
    "            + jnp.exp(1)\n",
    "            + theta2\n",
    "        )\n",
    "        if jnp.abs(loss - prev_loss) < convergence_criteria:\n",
    "            print(f\"Converged at iteration {i + 1}!\")\n",
    "            print(\n",
    "                f\"y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "            )\n",
    "            max_n_iterations = i\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        print(\n",
    "            f\"Iteration {(i + 1):4d}: y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    if return_history:\n",
    "        return {\n",
    "            \"theta1s\": theta1s,\n",
    "            \"theta2s\": theta2s,\n",
    "            \"losses\": losses,\n",
    "            \"f_preds\": f_preds,\n",
    "            \"max_n_iterations\": max_n_iterations,\n",
    "        }\n",
    "    else:\n",
    "        return theta1s[-1], theta2s[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d47b263023fd6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-23T01:24:52.607181Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = sgd(\n",
    "    lr,\n",
    "    max_n_iterations,\n",
    "    true_thetas,\n",
    "    loss_2d,\n",
    "    examples,\n",
    "    convergence_criteria,\n",
    "    key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_2d,\n",
    "    loss_2d,\n",
    "    true_thetas,\n",
    "    convergence_criteria,\n",
    "    sgd_history[\"theta1s\"],\n",
    "    sgd_history[\"theta2s\"],\n",
    "    sgd_history[\"losses\"],\n",
    "    sgd_history[\"f_preds\"],\n",
    "    sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57559343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_batched(\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "    max_n_iterations: int,\n",
    "    true_thetas: tuple[float | int, float | int],\n",
    "    loss_fn,\n",
    "    examples: list[tuple[float, float, float]],\n",
    "    convergence_criteria: float,\n",
    "    key,\n",
    "    return_history: bool = False,\n",
    ") -> dict | tuple[float, float]:\n",
    "    r\"\"\"Perform Stochastic Gradient Descent.\n",
    "\n",
    "    Args:\n",
    "        lr: The learning rate.\n",
    "        max_n_iterations: The maximum number of iterations.\n",
    "        true_thetas: The true thetas.\n",
    "        loss_fn: The loss function.\n",
    "        examples: The examples.\n",
    "        convergence_criteria: The convergence criteria.\n",
    "        key: The random key.\n",
    "        return_history: Whether to return the history.\n",
    "\n",
    "    Returns:\n",
    "        theta1s: The theta1 values recorded during optimization.\n",
    "        theta2s: The theta2 values recorded during optimization.\n",
    "        losses: The losses recorded during optimization.\n",
    "        f_preds: The predicted functions.\n",
    "        max_n_iterations: The maximum number of iterations\n",
    "\n",
    "        or\n",
    "\n",
    "        theta1: The final theta1 value.\n",
    "        theta2: The final theta2 value\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Running SGD with learning rate: {lr} and max iterations: {max_n_iterations}\"\n",
    "    )\n",
    "    print(f\"True thetas: {true_thetas}\")\n",
    "\n",
    "    loss_theta1 = grad(loss_fn, argnums=0)\n",
    "    loss_theta2 = grad(loss_fn, argnums=1)\n",
    "    theta1, theta2 = random.uniform(key), random.uniform(key)\n",
    "    prev_loss = 0\n",
    "    theta1s, theta2s, losses, f_preds = [], [], [], []\n",
    "    for i in range(max_n_iterations):\n",
    "        theta1_sum_loss, theta2_sum_loss = 0, 0\n",
    "        for _ in range(batch_size):\n",
    "            x, y, z = py_random.choices(examples)[0]\n",
    "            theta1_sum_loss += loss_theta1(theta1, theta2, x, y, z)\n",
    "            theta2_sum_loss += loss_theta2(theta1, theta2, x, y, z)\n",
    "\n",
    "        theta1_loss = theta1_sum_loss / batch_size\n",
    "        theta2_loss = theta2_sum_loss / batch_size\n",
    "\n",
    "        theta1 -= lr * theta1_loss\n",
    "        theta2 -= lr * theta2_loss\n",
    "        loss = loss_2d(theta1, theta2, x, y, z).item()\n",
    "        theta1s.append(theta1.item())\n",
    "        theta2s.append(theta2.item())\n",
    "        losses.append(loss)\n",
    "        f_preds.append(\n",
    "            lambda x, y, theta1, theta2: theta1 * x**2\n",
    "            + theta2 * y**2\n",
    "            + jnp.exp(1)\n",
    "            + theta2\n",
    "        )\n",
    "        if jnp.abs(loss - prev_loss) < convergence_criteria:\n",
    "            print(f\"Converged at iteration {i + 1}!\")\n",
    "            print(\n",
    "                f\"y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "            )\n",
    "            max_n_iterations = i\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        print(\n",
    "            f\"Iteration {(i + 1):4d}: y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    if return_history:\n",
    "        return {\n",
    "            \"theta1s\": theta1s,\n",
    "            \"theta2s\": theta2s,\n",
    "            \"losses\": losses,\n",
    "            \"f_preds\": f_preds,\n",
    "            \"max_n_iterations\": max_n_iterations,\n",
    "        }\n",
    "    else:\n",
    "        return theta1s[-1], theta2s[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = sgd_batched(\n",
    "    lr,\n",
    "    32,\n",
    "    max_n_iterations,\n",
    "    true_thetas,\n",
    "    loss_2d,\n",
    "    examples,\n",
    "    convergence_criteria,\n",
    "    key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_2d,\n",
    "    loss_2d,\n",
    "    true_thetas,\n",
    "    convergence_criteria,\n",
    "    sgd_history[\"theta1s\"],\n",
    "    sgd_history[\"theta2s\"],\n",
    "    sgd_history[\"losses\"],\n",
    "    sgd_history[\"f_preds\"],\n",
    "    sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03214774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSProp(\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    smoothing_constant: float,\n",
    "    momentum: float,\n",
    "    centered: bool,\n",
    "    max_n_iterations: int,\n",
    "    true_thetas: tuple[float | int, float | int],\n",
    "    loss_fn,\n",
    "    examples: list[tuple[float, float, float]],\n",
    "    convergence_criteria: float,\n",
    "    key,\n",
    "    return_history: bool = False,\n",
    ") -> dict | tuple[float, float]:\n",
    "    r\"\"\"Perform Stochastic Gradient Descent.\n",
    "\n",
    "    Algorithm taken from:\n",
    "    https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
    "\n",
    "    Args:\n",
    "        lr: The learning rate.\n",
    "        max_n_iterations: The maximum number of iterations.\n",
    "        true_thetas: The true thetas.\n",
    "        loss_fn: The loss function.\n",
    "        examples: The examples.\n",
    "        convergence_criteria: The convergence criteria.\n",
    "        key: The random key.\n",
    "        return_history: Whether to return the history.\n",
    "\n",
    "    Returns:\n",
    "        theta1s: The theta1 values recorded during optimization.\n",
    "        theta2s: The theta2 values recorded during optimization.\n",
    "        losses: The losses recorded during optimization.\n",
    "        f_preds: The predicted functions.\n",
    "        max_n_iterations: The maximum number of iterations\n",
    "\n",
    "        or\n",
    "\n",
    "        theta1: The final theta1 value.\n",
    "        theta2: The final theta2 value\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Running SGD with learning rate: {lr} and max iterations: {max_n_iterations}\"\n",
    "    )\n",
    "    print(f\"True thetas: {true_thetas}\")\n",
    "\n",
    "    loss_theta1 = grad(loss_fn, argnums=0)\n",
    "    loss_theta2 = grad(loss_fn, argnums=1)\n",
    "    theta1, theta2 = random.uniform(key), random.uniform(key)\n",
    "    prev_loss = 0\n",
    "    theta1s, theta2s, losses, f_preds = [], [], [], []\n",
    "    square_avg_1, square_avg_2 = 0, 0\n",
    "    buffer_1, buffer_2 = 0, 0\n",
    "    g_avg_1, g_avg_2 = 0, 0\n",
    "    for i in range(max_n_iterations):\n",
    "        x, y, z = py_random.choices(examples)[0]\n",
    "        g_t_1 = loss_theta1(theta1, theta2, x, y, z)\n",
    "        g_t_2 = loss_theta2(theta1, theta2, x, y, z)\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            g_t_1 += weight_decay * theta1\n",
    "            g_t_2 += weight_decay * theta2\n",
    "\n",
    "        square_avg_1 = smoothing_constant * square_avg_1 + (1 - lr) * g_t_1**2\n",
    "        square_avg_2 = smoothing_constant * square_avg_2 + (1 - lr) * g_t_2**2\n",
    "\n",
    "        square_avg_1_backup = square_avg_1\n",
    "        square_avg_2_backup = square_avg_2\n",
    "\n",
    "        if centered:\n",
    "            g_avg_1 = smoothing_constant * g_avg_1 + (1 - lr) * g_t_1\n",
    "            g_avg_2 = smoothing_constant * g_avg_2 + (1 - lr) * g_t_2\n",
    "            square_avg_1_backup -= g_avg_1**2\n",
    "            square_avg_2_backup -= g_avg_2**2\n",
    "\n",
    "        if momentum > 0:\n",
    "            buffer_1 = momentum * buffer_1 + g_t_1 / (\n",
    "                jnp.sqrt(square_avg_1_backup) + 1e-8\n",
    "            )\n",
    "            buffer_2 = momentum * buffer_2 + g_t_2 / (\n",
    "                jnp.sqrt(square_avg_2_backup) + 1e-8\n",
    "            )\n",
    "            theta1 -= lr * buffer_1\n",
    "            theta2 -= lr * buffer_2\n",
    "        else:\n",
    "            theta1 -= lr * g_t_1 / (jnp.sqrt(square_avg_1) + 1e-8)\n",
    "            theta2 -= lr * g_t_2 / (jnp.sqrt(square_avg_2) + 1e-8)\n",
    "\n",
    "        loss = loss_2d(theta1, theta2, x, y, z).item()\n",
    "        theta1s.append(theta1.item())\n",
    "        theta2s.append(theta2.item())\n",
    "        losses.append(loss)\n",
    "        f_preds.append(\n",
    "            lambda x, y, theta1, theta2: theta1 * x**2\n",
    "            + theta2 * y**2\n",
    "            + jnp.exp(1)\n",
    "            + theta2\n",
    "        )\n",
    "        if jnp.abs(loss - prev_loss) < convergence_criteria:\n",
    "            print(f\"Converged at iteration {i + 1}!\")\n",
    "            print(\n",
    "                f\"y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "            )\n",
    "            max_n_iterations = i\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        print(\n",
    "            f\"Iteration {(i + 1):4d}: y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    if return_history:\n",
    "        return {\n",
    "            \"theta1s\": theta1s,\n",
    "            \"theta2s\": theta2s,\n",
    "            \"losses\": losses,\n",
    "            \"f_preds\": f_preds,\n",
    "            \"max_n_iterations\": max_n_iterations,\n",
    "        }\n",
    "    else:\n",
    "        return theta1s[-1], theta2s[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc30530",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = RMSProp(\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    smoothing_constant=0.99,\n",
    "    momentum=0.9,\n",
    "    centered=False,\n",
    "    max_n_iterations=max_n_iterations,\n",
    "    true_thetas=true_thetas,\n",
    "    loss_fn=loss_2d,\n",
    "    examples=examples,\n",
    "    convergence_criteria=convergence_criteria,\n",
    "    key=key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_2d,\n",
    "    loss_2d,\n",
    "    true_thetas,\n",
    "    convergence_criteria,\n",
    "    sgd_history[\"theta1s\"],\n",
    "    sgd_history[\"theta2s\"],\n",
    "    sgd_history[\"losses\"],\n",
    "    sgd_history[\"f_preds\"],\n",
    "    sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd9931fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    betas: tuple[float, float],\n",
    "    momentum: float,\n",
    "    max_n_iterations: int,\n",
    "    true_thetas: tuple[float | int, float | int],\n",
    "    loss_fn,\n",
    "    examples: list[tuple[float, float, float]],\n",
    "    convergence_criteria: float,\n",
    "    key,\n",
    "    return_history: bool = False,\n",
    ") -> dict | tuple[float, float]:\n",
    "    r\"\"\"Perform Stochastic Gradient Descent.\n",
    "\n",
    "    Algorithm taken from:\n",
    "    https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
    "\n",
    "    Args:\n",
    "        lr: The learning rate.\n",
    "        max_n_iterations: The maximum number of iterations.\n",
    "        true_thetas: The true thetas.\n",
    "        loss_fn: The loss function.\n",
    "        examples: The examples.\n",
    "        convergence_criteria: The convergence criteria.\n",
    "        key: The random key.\n",
    "        return_history: Whether to return the history.\n",
    "\n",
    "    Returns:\n",
    "        theta1s: The theta1 values recorded during optimization.\n",
    "        theta2s: The theta2 values recorded during optimization.\n",
    "        losses: The losses recorded during optimization.\n",
    "        f_preds: The predicted functions.\n",
    "        max_n_iterations: The maximum number of iterations\n",
    "\n",
    "        or\n",
    "\n",
    "        theta1: The final theta1 value.\n",
    "        theta2: The final theta2 value\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Running SGD with learning rate: {lr} and max iterations: {max_n_iterations}\"\n",
    "    )\n",
    "    print(f\"True thetas: {true_thetas}\")\n",
    "\n",
    "    loss_theta1 = grad(loss_fn, argnums=0)\n",
    "    loss_theta2 = grad(loss_fn, argnums=1)\n",
    "    theta1, theta2 = random.uniform(key), random.uniform(key)\n",
    "    prev_loss = 0\n",
    "    theta1s, theta2s, losses, f_preds = [], [], [], []\n",
    "    first_moment_1, first_moment_2 = 0, 0\n",
    "    second_moment_1, second_moment_2 = 0, 0\n",
    "    v_max_1, v_max_2 = 0, 0\n",
    "    beta1, beta2 = betas\n",
    "    for i in range(max_n_iterations):\n",
    "        x, y, z = py_random.choices(examples)[0]\n",
    "        g_t_1 = loss_theta1(theta1, theta2, x, y, z)\n",
    "        g_t_2 = loss_theta2(theta1, theta2, x, y, z)\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            g_t_1 += weight_decay * theta1\n",
    "            g_t_2 += weight_decay * theta2\n",
    "\n",
    "        first_moment_1 = beta1 * first_moment_1 + (1 - beta1) * g_t_1\n",
    "        first_moment_2 = beta1 * first_moment_2 + (1 - beta1) * g_t_2\n",
    "\n",
    "        second_moment_1 = beta2 * second_moment_1 + (1 - beta2) * g_t_1**2\n",
    "        second_moment_2 = beta2 * second_moment_2 + (1 - beta2) * g_t_2**2\n",
    "\n",
    "        first_moment_1_hat = first_moment_1 / (1 - beta1)\n",
    "        first_moment_2_hat = first_moment_2 / (1 - beta1)\n",
    "\n",
    "        second_moment_1_hat = second_moment_1 / (1 - beta2)\n",
    "        second_moment_2_hat = second_moment_2 / (1 - beta2)\n",
    "\n",
    "        theta1 -= lr * first_moment_1_hat / (jnp.sqrt(second_moment_1_hat) + 1e-8)\n",
    "        theta2 -= lr * first_moment_2_hat / (jnp.sqrt(second_moment_2_hat) + 1e-8)\n",
    "\n",
    "        loss = loss_2d(theta1, theta2, x, y, z).item()\n",
    "        theta1s.append(theta1.item())\n",
    "        theta2s.append(theta2.item())\n",
    "        losses.append(loss)\n",
    "        f_preds.append(\n",
    "            lambda x, y, theta1, theta2: theta1 * x**2\n",
    "            + theta2 * y**2\n",
    "            + jnp.exp(1)\n",
    "            + theta2\n",
    "        )\n",
    "        if jnp.abs(loss - prev_loss) < convergence_criteria:\n",
    "            print(f\"Converged at iteration {i + 1}!\")\n",
    "            print(\n",
    "                f\"y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "            )\n",
    "            max_n_iterations = i\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        print(\n",
    "            f\"Iteration {(i + 1):4d}: y_pred = [{theta1:.3f}, {theta2:.3f}][x1^2, x2^2].T + theta2 + e, loss = {loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    if return_history:\n",
    "        return {\n",
    "            \"theta1s\": theta1s,\n",
    "            \"theta2s\": theta2s,\n",
    "            \"losses\": losses,\n",
    "            \"f_preds\": f_preds,\n",
    "            \"max_n_iterations\": max_n_iterations,\n",
    "        }\n",
    "    else:\n",
    "        return theta1s[-1], theta2s[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "max_n_iterations = 50\n",
    "convergence_criteria = 1e-2\n",
    "true_thetas = (3, 9)\n",
    "\n",
    "# Generate examples from the true function\n",
    "n_examples = 100\n",
    "examples = hp.generate_examples(f_2d, n_examples, key)\n",
    "\n",
    "sgd_history = Adam(\n",
    "    lr=lr,\n",
    "    weight_decay=0,\n",
    "    betas=(0.9, 0.999),\n",
    "    momentum=0.2,\n",
    "    max_n_iterations=max_n_iterations,\n",
    "    true_thetas=true_thetas,\n",
    "    loss_fn=loss_2d,\n",
    "    examples=examples,\n",
    "    convergence_criteria=convergence_criteria,\n",
    "    key=key,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "fig = hp.create_optimizer_figure_2d(\n",
    "    f_2d,\n",
    "    loss_2d,\n",
    "    true_thetas,\n",
    "    convergence_criteria,\n",
    "    sgd_history[\"theta1s\"],\n",
    "    sgd_history[\"theta2s\"],\n",
    "    sgd_history[\"losses\"],\n",
    "    sgd_history[\"f_preds\"],\n",
    "    sgd_history[\"max_n_iterations\"],\n",
    "    perf_profiling=False,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe740bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
